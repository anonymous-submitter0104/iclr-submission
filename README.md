# MILA: MULTILINGUAL INDIC LANGUAGE ARCHIVE

 _A smaller representative dataset has been uploaded in this repository. On acceptance of the paper, the full dataset will be released with proper legal licensing._

 ![Dataset Distribution](/readme-resources/token_distribution.png)

_A DATASET FOR EQUITABLE MULTILINGUAL LLMS_

Accompanying the Open review Submission [24079_MILA_MULTILINGUAL_INDIC_.pdf](https://github.com/anonymous-submitter0104/iclr-submission/blob/8308c89a777334fbeedba203fbb400c464961c60/24079_MILA_MULTILINGUAL_INDIC_.pdf) 

Long Technical Report Link: 

### Open Source Release

**We will Open Source the following:**

* **2T tokens** of high-quality pretraining data across **22 Scheduled Indian Languages**
* **300M image–text pairs** (spanning Indian languages, enabling Indic OCR and VLM development)
* **Indic Persona Hub (200M Indian virtual personas)**
* **India-centric parallel translated corpora** across **22 Scheduled Indian Languages**
* **Indic MMLU** benchmark covering 22 languages
* **Domain-specific Indian taxonomies**
* **High-quality web-crawled English**
* **Crawling and Scraping Pipelines**
* **First of its Kind Synthetic OCR Benchmark ISOB** across **22 Schedules Indian Languages**

_All releases will be hosted on Hugging Face after the review process with a accompanying license, in compliance with open-source best practices._

### Disclaimer

This repository is part of a research effort submitted to ICLR. 

Our objective is to **open-source large-scale Indian multilingual datasets** to strengthen the **open-source AI ecosystem** and promote **data sovereignty within the Indic AI community**.

All our pipelines are built entirely on **open-source models hosted on Hugging Face**, ensuring full transparency and reproducibility. We **do not use any closed-source LLMs, VLMs, or proprietary AI systems** at any stage of data creation or processing — a deliberate choice to uphold **data sovereignty and ethical independence** in our work.

If required, we are open to publishing benchmark comparisons against **closed-source or proprietary systems**. However, we have consciously chosen **not to use** such systems in our workflow.

This decision stems from our **commitment to ethical data handling** and our **partnership agreements** with multiple data providers. Using closed or proprietary systems for tasks like OCR, translation, or data processing would involve transferring sensitive data to external servers—**beyond the scope of our agreements**, and hence, would be **ethically inappropriate**.

To uphold these standards, we have developed **fully open-source pipelines** that can be **deployed locally on GPU infrastructure**, ensuring both transparency and data integrity throughout the process.

We intend to release not only the datasets but also the **data preparation recipes** and the accompanying open-source code. By doing so, we hope to enable the community to reproduce, extend, and innovate upon our methods in a fully transparent and sovereign manner.






