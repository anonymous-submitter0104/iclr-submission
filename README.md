# MILA: MULTILINGUAL INDIC LANGUAGE ARCHIVE
_A DATASET FOR EQUITABLE MULTILINGUAL LLMS_
 
_A smaller representative dataset has been uploaded in this repository. On acceptance of the paper, the full dataset will be released with proper legal licensing._

 ![Dataset Distribution](/readme-resources/token_distribution.png)

Accompanying the Open review Submission [24079_MILA_MULTILINGUAL_INDIC_.pdf](https://github.com/anonymous-submitter0104/iclr-submission/blob/main/24079_MILA_MULTILINGUAL_INDIC_.pdf) 

Long Technical Report Link: [Full Technical Report (version-1) _Final Version Coming Soon!_](https://github.com/anonymous-submitter0104/iclr-submission/blob/main/Data%20Preparation%20Technical%20Report.pdf)

### Open Source Release

**We will Open Source the following:**

* **2T tokens** of high-quality pretraining data across **22 Scheduled Indian Languages**
* **300M image–text pairs** (spanning Indian languages, enabling Indic OCR and VLM development)
* **Indic Persona Hub (200M Indian virtual personas)**
* **India-centric parallel translated corpora** across **22 Scheduled Indian Languages**
* **Indic MMLU** benchmark covering 22 languages
* **Domain-specific Indian taxonomies**
* **High-quality web-crawled English**
* **Crawling and Scraping Pipelines**
* **First of its Kind Synthetic OCR Benchmark ISOB** across **22 Schedules Indian Languages**

_All releases will be hosted on Hugging Face after the review process with a accompanying license, in compliance with open-source best practices._

### Disclaimer

This repository is part of a research effort submitted to ICLR. 

Our objective is to **open-source large-scale Indian multilingual datasets** to strengthen the **open-source AI ecosystem** and promote **data sovereignty within the Indic AI community**.

All our pipelines are built entirely on **open-source models from Hugging Face**, ensuring full transparency and reproducibility. We **do not use any closed-source LLMs, VLMs, or proprietary AI systems** at any stage of data creation or processing — a deliberate choice to uphold **data sovereignty and ethical independence** in our work.

If required, we are open to publishing benchmark comparisons against **closed-source or proprietary systems**. However, we have consciously chosen **not to use** such systems in our workflow.

This decision stems from our **commitment to ethical data handling** and our **partnership agreements** with multiple data providers. Using closed or proprietary systems for tasks like OCR, translation, or data processing would involve transferring sensitive data to external servers—**beyond the scope of our agreements**, and hence, would be **ethically inappropriate**.

To uphold these standards, we have developed **fully open-source pipelines** that can be **deployed locally on GPU infrastructure**, ensuring both transparency and data integrity throughout the process.

We intend to release not only the **datasets** but also the **data preparation recipes** and the accompanying open-source code. By doing so, we hope to enable the community to reproduce, extend, and innovate upon our methods in a fully transparent and sovereign manner.


### ❤️ A Note from the Authors

This work represents a **fundamentally foundational step** in what we believe to be one of the most important and underexplored areas of modern AI / Generative AI — **data preparation for training large language models**. While often considered an auxiliary process, our extensive experiments and ablation studies reaffirm that **careful data preparation is not just a prerequisite but a cornerstone** for building performant and responsible LLMs.

Through our experience, we have come to realize that **data preparation for pretraining remains one of the most complex, least standardized, and least openly studied components** of the LLM pipeline. Despite being central to every foundation model’s success, **there exists no peer-reviewed framework or systematically validated body of work** in this space.

Our intent with this submission is to **broaden the conversation on what constitutes valuable and rigorous scientific contribution** in AI. By presenting this work in a **peer-reviewed, open-source setting**, we hope to **encourage transparency, reproducibility, and community validation**, and to inspire other research groups and top-tier labs to bring similar efforts into the open domain.

We firmly believe that **progress in generative AI should be built on openness, shared validation, and collective growth**. If this work helps spark even a small shift toward greater transparency in data processes and benchmarking, we would consider it a meaningful step forward — for the community, and for the collective progress of humanity in AI.








